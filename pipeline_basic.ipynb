{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (5.0.8)\n",
      "Requirement already satisfied: tabulate in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: haystack-ai in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: google-ai-haystack in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: sourcegraph==0.0.6 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (0.0.6)\n",
      "Requirement already satisfied: google-generativeai in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (0.7.2)\n",
      "Requirement already satisfied: matplotlib~=3.8.4 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from sourcegraph==0.0.6) (3.8.4)\n",
      "Requirement already satisfied: networkx~=3.2.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from sourcegraph==0.0.6) (3.2.1)\n",
      "Requirement already satisfied: haystack-experimental in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (0.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (3.1.2)\n",
      "Requirement already satisfied: lazy-imports in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (0.3.1)\n",
      "Requirement already satisfied: more-itertools in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (10.3.0)\n",
      "Requirement already satisfied: numpy<2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (1.37.1)\n",
      "Requirement already satisfied: pandas in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (2.2.2)\n",
      "Requirement already satisfied: posthog in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (8.5.0)\n",
      "Requirement already satisfied: tqdm in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from haystack-ai) (4.8.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (0.6.6)\n",
      "Requirement already satisfied: google-api-core in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (2.19.1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (2.139.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (2.32.0)\n",
      "Requirement already satisfied: protobuf in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (5.27.3)\n",
      "Requirement already satisfied: pydantic in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.63.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from matplotlib~=3.8.4->sourcegraph==0.0.6) (3.1.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from requests->haystack-ai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from requests->haystack-ai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from requests->haystack-ai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from requests->haystack-ai) (2023.7.22)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from jinja2->haystack-ai) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from pandas->haystack-ai) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from pandas->haystack-ai) (2024.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from posthog->haystack-ai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from posthog->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.65.4)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.62.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/ransaka/miniconda3/envs/learning/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
      "Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.27.3\n",
      "    Uninstalling protobuf-5.27.3:\n",
      "      Successfully uninstalled protobuf-5.27.3\n",
      "Successfully installed protobuf-4.25.4\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install redis tabulate haystack-ai google-ai-haystack sourcegraph==0.0.6 google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install --upgrade --force-reinstall protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import redis\n",
    "from typing import List\n",
    "from redis.commands.search.query import Query\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = redis.Redis(\n",
    "  host=os.environ['REDIS_HOST'],\n",
    "  port=12305,\n",
    "  password=os.environ['REDIS_PASSWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "def get_embeddings(content: List):\n",
    "    return genai.embed_content(model='models/text-embedding-004',content=content)['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Training new tokenizer takes a lot time to complete. Also memory consumption seems pretty high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_prompt(query: str, chat_history: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search and retrieve related functions.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query to encode.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing details of related functions.\n",
    "    \"\"\"\n",
    "    INDEX_NAME = \"idx:codes_vss\"\n",
    "    \n",
    "    vector_search_query = (\n",
    "        Query('(*)=>[KNN 2 @vector $query_vector AS vector_score]')\n",
    "        .sort_by('vector_score')\n",
    "        .return_fields('vector_score', 'id', 'name', 'definition', 'file_name', 'type', 'uses')\n",
    "        .dialect(2)\n",
    "    )\n",
    "    \n",
    "    encoded_query = get_embeddings(query)\n",
    "    vector_params = {\n",
    "        \"query_vector\": np.array(encoded_query, dtype=np.float32).tobytes()\n",
    "    }\n",
    "    \n",
    "    result_docs = client.ft(INDEX_NAME).search(vector_search_query, vector_params).docs\n",
    "    \n",
    "    related_items: List[str] = []\n",
    "    dependencies: List[str] = []\n",
    "    for doc in result_docs:\n",
    "        related_items.append(doc.name)\n",
    "        if doc.uses:\n",
    "            dependencies.extend(use for use in doc.uses.split(\", \") if use)\n",
    "    \n",
    "    dependencies = list(set(dependencies) - set(related_items))\n",
    "    \n",
    "    def get_query(item_list):\n",
    "        return Query(f\"@name:({' | '.join(item_list)})\").return_fields(\n",
    "            'id', 'name', 'definition', 'file_name', 'type'\n",
    "        )\n",
    "    \n",
    "    related_docs = client.ft(INDEX_NAME).search(get_query(related_items)).docs\n",
    "    dependency_docs = client.ft(INDEX_NAME).search(get_query(dependencies)).docs\n",
    "    \n",
    "    def format_doc(doc):\n",
    "        return (\n",
    "            f\"{'*' * 28} CODE SNIPPET {doc.id} {'*' * 28}\\n\"\n",
    "            f\"* Name: {doc.name}\\n\"\n",
    "            f\"* File: {doc.file_name}\\n\"\n",
    "            f\"* {doc.type.capitalize()} definition:\\n\"\n",
    "            f\"```python\\n{doc.definition}\\n```\\n\"\n",
    "        )\n",
    "    \n",
    "    formatted_results_main = [format_doc(doc) for doc in related_docs]\n",
    "    formatted_results_support = [format_doc(doc) for doc in dependency_docs]\n",
    "    \n",
    "    return (\n",
    "        f\"User Question: {query}\\n\\n\"\n",
    "        f\"Current Chat History: \\n{chat_history}\\n\\n\"\n",
    "        f\"USE BELOW CODES TO ANSWER USER QUESTIONS.\\n\"\n",
    "        f\"{chr(10).join(formatted_results_main)}\\n\\n\"\n",
    "        f\"SOME SUPPORTING FUNCTIONS AND CLASS YOU MAY WANT.\\n\"\n",
    "        f\"{chr(10).join(formatted_results_support)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723259930.726312 1717903 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "prompt = draft_prompt(query,{\"user\":\"hi\",\"Agent\":\"hello there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: Training new tokenizer takes a lot time to complete. Also memory consumption seems pretty high\n",
      "\n",
      "Current Chat History: \n",
      "{'user': 'hi', 'Agent': 'hello there'}\n",
      "\n",
      "USE BELOW CODES TO ANSWER USER QUESTIONS.\n",
      "**************************** CODE SNIPPET code:004 ****************************\n",
      "* Name: load_tokenizer\n",
      "* File: dataset_utils.py\n",
      "* Function definition:\n",
      "```python\n",
      "def load_tokenizer():\n",
      "    tokenizer = Tokenizer(max_length=MAX_LENGTH)\n",
      "    tokenizer.load_from_pretrained(DUMMY_FILE_NAME)\n",
      "    return tokenizer\n",
      "```\n",
      "\n",
      "**************************** CODE SNIPPET code:002 ****************************\n",
      "* Name: Tokenizer\n",
      "* File: tokenizer.py\n",
      "* Class definition:\n",
      "```python\n",
      "class Tokenizer:\n",
      "\n",
      "    def __init__(self, max_length: int, unknown_token: str='<unk>', pad_token: str='<pad>'):\n",
      "        self.unknown_token_id = None\n",
      "        self.token_id_to_token_map = None\n",
      "        self.vocab_map = None\n",
      "        self.unknown_token = unknown_token\n",
      "        self.pad_token = pad_token\n",
      "        self.tokenized_chars = []\n",
      "        self.unique_chars = []\n",
      "        self.special_tokens = [self.unknown_token, self.pad_token]\n",
      "        self.max_length = max_length\n",
      "        self.pad_token_id = None\n",
      "\n",
      "    def __encode(self, text, truncate_and_pad: bool) -> list:\n",
      "        processed_text = self.__process_text(text)\n",
      "        text_encodings = [self.vocab_map.get(char, self.unknown_token_id) for char in processed_text]\n",
      "        if truncate_and_pad:\n",
      "            return self.pad_or_truncate(sequence=text_encodings, max_length=self.max_length, padding_value=self.pad_token_id)\n",
      "        else:\n",
      "            return text_encodings\n",
      "\n",
      "    @staticmethod\n",
      "    def pad_or_truncate(sequence, max_length, padding_value):\n",
      "        if len(sequence) > max_length:\n",
      "            return sequence[:max_length]\n",
      "        elif len(sequence) < max_length:\n",
      "            return sequence + [padding_value] * (max_length - len(sequence))\n",
      "        else:\n",
      "            return sequence\n",
      "\n",
      "    def __call__(self, text, truncate_and_pad: bool=True) -> list:\n",
      "        \"\"\"\n",
      "        Encode the given text into a list of tokens.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        text : str\n",
      "            Text to be encoded.\n",
      "        truncate_and_pad: bool\n",
      "            Set as True if you need to truncate/pad encodings False otherwise\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        encoded_tokens : list of int\n",
      "            List of tokens representing the encoded text.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sinlib import Tokenizer\n",
      "        >>> corpus = [...]\n",
      "        >>> tokenizer = Tokenizer()\n",
      "        >>> tokenizer.train(corpus)\n",
      "        >>> tokenizer(\"මම ගෙදර ගියා\")\n",
      "        [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\n",
      "        \"\"\"\n",
      "        return self.__encode(text, truncate_and_pad=truncate_and_pad)\n",
      "\n",
      "    def decode(self, ids, skip_special_tokens: bool=False) -> str:\n",
      "        \"\"\"\n",
      "        Decode a list of token IDs into a string.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        ids : list of int\n",
      "            List of token IDs to be decoded.\n",
      "        skip_special_tokens: bool\n",
      "            Whether to consider special tokens when decoding sequences\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        decoded_text : str\n",
      "            The decoded text string.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sinlib import Tokenizer\n",
      "        >>> tokenizer = Tokenizer()\n",
      "        >>> tokenizer.train([...])\n",
      "        >>> encoded_tokens = [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\n",
      "        >>> tokenizer.decode(encoded_tokens)\n",
      "        'මම ගෙදර ගියා'\n",
      "        \"\"\"\n",
      "        special_token_ids = [self.vocab_map[tok] for tok in self.special_tokens]\n",
      "        if skip_special_tokens:\n",
      "            return ''.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids if token not in special_token_ids])\n",
      "        else:\n",
      "            return ''.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids])\n",
      "\n",
      "    def train(self, text_list) -> None:\n",
      "        \"\"\"\n",
      "        Train the tokenizer on a list of text strings.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        text_list : list of str\n",
      "            List of text strings to be used for training the tokenizer.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sinlib import Tokenizer\n",
      "        >>> corpus = [...]\n",
      "        >>> tokenizer = Tokenizer()\n",
      "        >>> tokenizer.train(corpus)\n",
      "        \"\"\"\n",
      "        self.__train_character_level_tokenizer(text_list)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.vocab_map)\n",
      "\n",
      "    @property\n",
      "    def vocab_size(self):\n",
      "        return len(self)\n",
      "\n",
      "    @staticmethod\n",
      "    def __process_text(t):\n",
      "        return process_text(t)\n",
      "\n",
      "    def __train_character_level_tokenizer(self, text_list):\n",
      "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
      "            results = list(executor.map(self.__process_text, text_list))\n",
      "            self.tokenized_chars = [char for sublist in results for char in sublist]\n",
      "        self.unique_chars = set(self.tokenized_chars)\n",
      "        self.vocab_map = dict(zip(self.unique_chars, range(len(self.unique_chars))))\n",
      "        self.vocab_map[self.unknown_token] = len(self.vocab_map)\n",
      "        self.vocab_map[self.pad_token] = len(self.vocab_map)\n",
      "        self.unknown_token_id = self.vocab_map[self.unknown_token]\n",
      "        self.pad_token_id = self.vocab_map[self.pad_token]\n",
      "        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\n",
      "\n",
      "    def load_from_pretrained(self, file_path: str) -> None:\n",
      "        \"\"\"\n",
      "        Load the vocabulary map from a pre-trained file.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        file_path : str\n",
      "            Path to the file containing the pre-trained vocabulary map.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        None\n",
      "\n",
      "        Warns\n",
      "        -----\n",
      "        UserWarning\n",
      "            If the file is not found at the specified path, a default vocabulary map is loaded and a warning is issued.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sinlib import Tokenizer\n",
      "        >>> tokenizer = Tokenizer()\n",
      "        >>> tokenizer.load_from_pretrained(\"pretrained_vocab.json\")\n",
      "        \"\"\"\n",
      "        file_path = Path(file_path)\n",
      "        if file_path.exists():\n",
      "            with open(file_path / 'vocab.json', 'r') as f:\n",
      "                self.vocab_map = json.load(f)\n",
      "            with open(file_path / 'config.json', 'r') as f:\n",
      "                configurations = json.load(f)\n",
      "            self.unknown_token = configurations['unknown_token']\n",
      "            self.pad_token = configurations['pad_token']\n",
      "            self.unknown_token_id = configurations['unknown_token_id']\n",
      "            self.pad_token_id = configurations['pad_token_id']\n",
      "            self.max_length = configurations['max_length']\n",
      "        else:\n",
      "            warnings.warn('File not found at the specified path. Loaded default vocab map.', UserWarning)\n",
      "            self.vocab_map = load_default_vocab_map()\n",
      "        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\n",
      "        self.unknown_token_id = self.vocab_map[self.unknown_token]\n",
      "        self.pad_token_id = self.vocab_map[self.pad_token]\n",
      "        return self\n",
      "\n",
      "    def save_tokenizer(self, save_path: str):\n",
      "        save_path = Path(save_path)\n",
      "        configurations = {'unknown_token': self.unknown_token, 'pad_token': self.pad_token, 'unknown_token_id': self.unknown_token_id, 'pad_token_id': self.pad_token_id, 'max_length': self.max_length}\n",
      "        with open(save_path / 'vocab.json', 'w', encoding='utf-8') as file:\n",
      "            json.dump(self.vocab_map, file, ensure_ascii=False, indent=4)\n",
      "        with open(save_path / 'config.json', 'w') as file:\n",
      "            json.dump(configurations, file, indent=4)\n",
      "```\n",
      "\n",
      "\n",
      "SOME SUPPORTING FUNCTIONS AND CLASS YOU MAY WANT.\n",
      "**************************** CODE SNIPPET code:010 ****************************\n",
      "* Name: load_default_vocab_map\n",
      "* File: preprocessing.py\n",
      "* Function definition:\n",
      "```python\n",
      "def load_default_vocab_map():\n",
      "    with open(Path(DEFAULT_VOCAB_MAP_FP) / 'vocab.json', 'r') as f:\n",
      "        vocab_map = json.load(f)\n",
      "    return vocab_map\n",
      "```\n",
      "\n",
      "**************************** CODE SNIPPET code:013 ****************************\n",
      "* Name: process_text\n",
      "* File: preprocessing.py\n",
      "* Function definition:\n",
      "```python\n",
      "def process_text(t):\n",
      "    tokenized_chars = []\n",
      "    for i, char in enumerate(t):\n",
      "        if char in VOWEL_DIACRITICS:\n",
      "            continue\n",
      "        if char in NUBERS_AND_PUNKTS:\n",
      "            tokenized_chars.append(char)\n",
      "        elif char == ' ':\n",
      "            tokenized_chars.append(' ')\n",
      "        elif char in ALL_LETTERS:\n",
      "            if i < len(t) - 1 and t[i + 1] in ALL_LETTERS:\n",
      "                tokenized_chars.append(char)\n",
      "            elif i < len(t) - 1 and t[i + 1] in VOWEL_DIACRITICS:\n",
      "                tokenized_chars.append(char + t[i + 1])\n",
      "            else:\n",
      "                tokenized_chars.append(char)\n",
      "        else:\n",
      "            tokenized_chars.append(char)\n",
      "    return tokenized_chars\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack import component\n",
    "from haystack.utils import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack_integrations.components.generators.google_ai import GoogleAIGeminiGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class RedisRetreiver:\n",
    "  @component.output_types(context=str)\n",
    "  def run(self, query:str, chat_history:str):\n",
    "    return {\"context\": draft_prompt(query, chat_history)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleAIGeminiGenerator(api_key=Secret.from_env_var(\"GEMINI_API_KEY\"), model='gemini-1.5-pro')\n",
    "# llm = OpenAIGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful agent optimized to resolve GitHub issues for your organization's libraries. Users will ask questions when they encounter problems with the code repository.\n",
    "You have access to all the necessary code for addressing these issues. \n",
    "First, you should understand the user's question and identify the relevant code blocks. \n",
    "Then, craft a precise and targeted response that allows the user to find an exact solution to their problem. \n",
    "You must provide code snippets rather than just opinions.\n",
    "You should always assume user has installed this python package in their system and raised question raised while they are using the library.\n",
    "\n",
    "In addition to the above tasks, you are free to:\n",
    " * Greet the user.\n",
    " * [ONLY IF THE QUESTION IS INSUFFICIENT] Request additional clarity.\n",
    " * Politely decline irrelevant queries.\n",
    " * Inform the user if their query cannot be processed or accomplished.\n",
    "\n",
    "By any chance you should NOT,\n",
    " * Ask or recommend user to use different library. Or code snipits related to other similar libraies.\n",
    " * Provide inaccurate explnations.\n",
    " * Provide sugestions without code examples.\n",
    "\n",
    "{{context}}\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(name=\"retriever\", instance=RedisRetreiver())\n",
    "pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "pipeline.add_component(\"llm\", llm)\n",
    "pipeline.connect(\"retriever.context\", \"prompt_builder\")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "pipeline.draw(path='pipeline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training new tokenizer takes a lot time to complete. Also memory consumption seems pretty high'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! I see that you are facing issues with training a new tokenizer that takes a lot of time to complete and results in high memory consumption. To address this, you can optimize the training process by specifying a smaller corpus for training.\n",
       "\n",
       "You can try the following code snippet in your training process to reduce the memory consumption:\n",
       "\n",
       "```python\n",
       "from sinlib import Tokenizer\n",
       "\n",
       "# Load the tokenizer\n",
       "tokenizer = Tokenizer(max_length=MAX_LENGTH)\n",
       "\n",
       "# Train the tokenizer on a subset of the corpus\n",
       "corpus_subset = [...]  # Provide a smaller subset of your corpus\n",
       "tokenizer.train(corpus_subset)\n",
       "\n",
       "# Encode/decode using the trained tokenizer\n",
       "encoded_text = tokenizer(\"Your input text here\")\n",
       "decoded_text = tokenizer.decode(encoded_text)\n",
       "```\n",
       "\n",
       "By training the tokenizer on a smaller corpus, you can reduce the time taken for training and lower the memory consumption. Let me know if you need further assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Training new tokenizer takes a lot time to complete. Also memory consumption seems pretty high'\n",
    "response = pipeline.run({\"retriever\": {\"query\": query, \"chat_history\": {}}})\n",
    "Markdown(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! To train a Tokenizer for a new dataset, you need to follow these steps:\n",
       "\n",
       "1. Load the Tokenizer:\n",
       "```python\n",
       "from sinlib import Tokenizer\n",
       "\n",
       "tokenizer = Tokenizer()\n",
       "```\n",
       "\n",
       "2. Read and preprocess the text from your dataset (corpus.txt):\n",
       "```python\n",
       "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
       "    lines = file.readlines()\n",
       "\n",
       "corpus = [line.strip() for line in lines]\n",
       "```\n",
       "\n",
       "3. Train the Tokenizer on this corpus:\n",
       "```python\n",
       "tokenizer.train(corpus)\n",
       "```\n",
       "\n",
       "4. Save the trained Tokenizer for future use:\n",
       "```python\n",
       "save_path = \"path_to_save\"\n",
       "tokenizer.save_tokenizer(save_path)\n",
       "```\n",
       "\n",
       "By following these steps, your Tokenizer will be trained on the new dataset from corpus.txt. Let me know if you need any more assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Give me example of training Tokenizer for new dataset. My dataset contains text lines. It's name is corpus.txt\"\n",
    "response = pipeline.run({\"retriever\": {\"query\": question, \"chat_history\": {}}})\n",
    "Markdown(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! It seems like your training process for the tokenizer is taking a lot of time. One potential reason for this could be the character level tokenization process that might get slow with large amounts of text data. \n",
       "\n",
       "To optimize this process, you can consider the following modifications:\n",
       "\n",
       "1. **Reduce Text Size**: If possible, try training the tokenizer on a smaller subset of the text data to see if the training time improves.\n",
       "\n",
       "2. **Multithreading**: The tokenizer training process is already set up to run using multiple threads for faster processing. You can further optimize this by adjusting the number of threads based on your system's capabilities.\n",
       "\n",
       "3. **Batch Processing**: Instead of processing the entire text corpus at once, consider breaking it down into smaller batches for training.\n",
       "\n",
       "Here's a sample code snippet on how you can adjust the number of threads for multithreading in the `Tokenizer` class:\n",
       "\n",
       "```python\n",
       "import concurrent.futures\n",
       "\n",
       "...\n",
       "\n",
       "def train(self, text_list) -> None:\n",
       "    \"\"\"\n",
       "    Train the tokenizer on a list of text strings.\n",
       "\n",
       "    Parameters\n",
       "    ----------\n",
       "    text_list : list of str\n",
       "        List of text strings to be used for training the tokenizer.\n",
       "\n",
       "    Examples\n",
       "    --------\n",
       "    >>> from sinlib import Tokenizer\n",
       "    >>> corpus = [...]\n",
       "    >>> tokenizer = Tokenizer()\n",
       "    >>> tokenizer.train(corpus)\n",
       "    \"\"\"\n",
       "    max_threads = 4  # Adjust the number of threads as per your system\n",
       "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
       "        results = list(executor.map(self.__process_text, text_list))\n",
       "        self.tokenized_chars = [char for sublist in results for char in sublist]\n",
       "    self.unique_chars = set(self.tokenized_chars)\n",
       "    self.vocab_map = dict(zip(self.unique_chars, range(len(self.unique_chars))))\n",
       "    self.vocab_map[self.unknown_token] = len(self.vocab_map)\n",
       "    self.vocab_map[self.pad_token] = len(self.vocab_map)\n",
       "    self.unknown_token_id = self.vocab_map[self.unknown_token]\n",
       "    self.pad_token_id = self.vocab_map[self.pad_token]\n",
       "    self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\n",
       "```\n",
       "\n",
       "Try adjusting the `max_threads` value based on your system's capacity. This should help optimize the training process and potentially reduce the time it takes to train the new tokenizer. Let me know if you need further assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Training new tokenizer take forever to finish.\"\n",
    "response = pipeline.run({\"retriever\": {\"query\": question, \"chat_history\": {}}}, include_outputs_from=['retriever'])\n",
    "Markdown(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! To train a new transliterator model for a new language, you can follow these steps:\n",
       "\n",
       "Step 1: Create a new Tokenizer for the new language\n",
       "```python\n",
       "new_language_tokenizer = Tokenizer(max_length=MAX_LENGTH)\n",
       "new_language_tokenizer.train(training_corpus)\n",
       "```\n",
       "\n",
       "Step 2: Update the load_transliterator_model function to load the new model\n",
       "```python\n",
       "def load_transliterator_model():\n",
       "    tokenizer = new_language_tokenizer\n",
       "    input_size = len(tokenizer)\n",
       "    output_size = len(tokenizer)\n",
       "    hidden_size = HIDDEN_SIZE\n",
       "    filepath = Path(MODELS_PATH) / NEW_CHECKPOINT_NAME\n",
       "    device = detect_device()\n",
       "    model = BiLSTMTranslator(input_size, hidden_size, output_size).to(device)\n",
       "    checkpoint = torch.load(filepath, map_location=device)\n",
       "    model.load_state_dict(checkpoint)\n",
       "    return model\n",
       "```\n",
       "\n",
       "Step 3: Train the new transliterator model for the new language\n",
       "```python\n",
       "transliterator = Transliterator()\n",
       "new_language_corpus = get_training_corpus_for_new_language()\n",
       "new_language_tokenizer.train(new_language_corpus)\n",
       "new_language_transliterator_model = train_new_model(new_language_tokenizer)  # Function not provided, should be implemented based on your training methodology\n",
       "```\n",
       "\n",
       "By following these steps, you should be able to train a new transliterator model for a new language. Let me know if you need further assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How to train new transilator model for new language?\"\n",
    "response = pipeline.run({\"retriever\": {\"query\": question, \"chat_history\": {}}}, include_outputs_from=['retriever'])\n",
    "Markdown(response[\"llm\"][\"replies\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
