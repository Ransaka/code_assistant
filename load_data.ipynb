{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import redis\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from redis.commands.search.field import (\n",
    "    TagField,\n",
    "    TextField,\n",
    "    VectorField,\n",
    ")\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = redis.Redis(\n",
    "  host=os.environ['REDIS_HOST'],\n",
    "  port=12305,\n",
    "  password=os.environ['REDIS_PASSWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sourcegraph import Sourcegraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"You are optimized to generate accurate descriptions for given Python codes. When the user inputs the code, you must return the description according to its goal and functionality.  You are not allowed to generate additional details. The user expects at least 5 sentence-long descriptions.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url):\n",
    "    def get_description(code):\n",
    "      chat_session = model.start_chat(\n",
    "        history=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [\n",
    "              f\"Code: {code}\",\n",
    "            ],\n",
    "          },\n",
    "        ]\n",
    "      )\n",
    "      response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "\n",
    "      return response.text\n",
    "    gihub_repository = Sourcegraph(url)\n",
    "    gihub_repository.run()\n",
    "    data = dict(gihub_repository.node_data)\n",
    "    for key, value in tqdm(data.items()):\n",
    "      data[key]['description'] = get_description(value['definition'])\n",
    "      data[key]['uses'] = \", \".join(list(gihub_repository.get_dependencies(key)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723182195.471295 1337262 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "100%|██████████| 15/15 [00:24<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "data = fetch_data(\"https://github.com/Ransaka/sinlib.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'class',\n",
       " 'name': 'Romanizer',\n",
       " 'definition': \"class Romanizer:\\n\\n    def __init__(self, char_mapper_fp: str, tokenizer_path: str):\\n        if char_mapper_fp is None:\\n            char_mapper_fp = CHAR_MAPPER_FP\\n        if tokenizer_path is None:\\n            tokenizer_path = DEFAULT_VOCAB_MAP_FP\\n        self.char_mapper = load_char_mapper(char_mapper_fp)\\n        self.tokenizer = Tokenizer(max_length=None)\\n        self.tokenizer.load_from_pretrained(tokenizer_path)\\n\\n    def __call__(self, text):\\n        return self.__romanize(text)\\n\\n    def __romanize(self, text: str):\\n        text = remove_non_printable(text)\\n        chars = np.array(list(text))\\n        sinhala_mask = [True if ch in ALL_SINHALA_CHARACTERS + list(NUBERS_AND_PUNKTS) + [' '] else False for ch in chars]\\n        sinhala_text = ''.join(chars[sinhala_mask]).strip()\\n        encodings = self.tokenizer(sinhala_text, truncate_and_pad=False)\\n        decoded_sinhala_chars = [self.tokenizer.token_id_to_token_map[c] for c in encodings]\\n        romanized_sinhala = [self.char_mapper.get(ch, ch if ch in NUBERS_AND_PUNKTS.union(' ') else '') for ch in decoded_sinhala_chars]\\n        romanized_sinhala = ''.join(romanized_sinhala)\\n        word_2_word_mapping = dict(zip(sinhala_text.split(), romanized_sinhala.split()))\\n        romanized_text = [word_2_word_mapping.get(word, word) for word in text.split()]\\n        return ' '.join(romanized_text)\",\n",
       " 'file_name': 'romanize.py',\n",
       " 'docstring': '',\n",
       " 'description': 'The provided Python code defines a class named `Romanizer` that is designed to romanize Sinhala text. It takes a character mapper file path and a tokenizer path as input during initialization. The class utilizes a character mapper to convert Sinhala characters to their romanized equivalents and a tokenizer to process the input text. The `__call__` method serves as a wrapper for the `__romanize` method, which performs the actual romanization process. The `__romanize` method removes non-printable characters from the input text, extracts Sinhala characters, tokenizes them, converts them to their romanized forms using the character mapper, and finally returns the romanized text by mapping words in the original text to their romanized counterparts. \\n',\n",
       " 'uses': 'process_text, load_default_vocab_map, remove_non_printable, Tokenizer, load_char_mapper'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Romanizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723202734.831485 1337262 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001 Obtain a distributed representation of a text.\n",
      "models/text-embedding-004 Obtain a distributed representation of a text.\n"
     ]
    }
   ],
   "source": [
    "for model_details in list(genai.list_models()):\n",
    "    if 'embedContent' in model_details.supported_generation_methods:\n",
    "        print(model_details.name,model_details.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(content: List):\n",
    "    return genai.embed_content(model='models/text-embedding-004',content=content)['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"idx:codes_vss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(data):\n",
    "    pipeline = client.pipeline()\n",
    "    for i, code_metadata in enumerate(data.values(), start=1):\n",
    "        redis_key = f\"code:{i:03}\"\n",
    "        pipeline.json().set(redis_key, \"$\", code_metadata)\n",
    "    _ = pipeline.execute()\n",
    "    keys = sorted(client.keys(\"code:*\"))\n",
    "    defs = client.json().mget(keys, \"$.definition\")\n",
    "    descs = client.json().mget(keys, \"$.description\")\n",
    "    embed_inputs = []\n",
    "\n",
    "    for i in range(1, len(keys)+1):\n",
    "        embed_inputs.append(\n",
    "            f\"\"\"{defs[i-1][0]}\\n\\n{descs[i-1][0]}\"\"\"\n",
    "        )\n",
    "    embeddings = get_embeddings(embed_inputs)\n",
    "    VECTOR_DIMENSION = len(embeddings[0])\n",
    "    pipeline = client.pipeline()\n",
    "    for key, embedding in zip(keys, embeddings):\n",
    "        pipeline.json().set(key, \"$.embeddings\", embedding)\n",
    "    pipeline.execute()\n",
    "\n",
    "    schema = (\n",
    "        TextField(\"$.name\", no_stem=True, as_name=\"name\"),\n",
    "        TagField(\"$.type\", as_name=\"type\"),\n",
    "        TextField(\"$.definition\", no_stem=True, as_name=\"definition\"),\n",
    "        TextField(\"$.file_name\", no_stem=True, as_name=\"file_name\"),\n",
    "        TextField(\"$.description\", no_stem=True, as_name=\"description\"),\n",
    "        TextField(\"$.uses\", no_stem=True, as_name=\"uses\"),\n",
    "        VectorField(\n",
    "            \"$.embeddings\",\n",
    "            \"HNSW\",\n",
    "            {\n",
    "                \"TYPE\": \"FLOAT32\",\n",
    "                \"DIM\": VECTOR_DIMENSION,\n",
    "                \"DISTANCE_METRIC\": \"COSINE\",\n",
    "            },\n",
    "            as_name=\"vector\",\n",
    "        ),\n",
    "    )\n",
    "    definition = IndexDefinition(prefix=[\"code:\"], index_type=IndexType.JSON)\n",
    "    _ = client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)\n",
    "\n",
    "    info = client.ft(INDEX_NAME).info()\n",
    "    num_docs = info[\"num_docs\"]\n",
    "    indexing_failures = info[\"hash_indexing_failures\"]\n",
    "    print(f\"{num_docs} documents indexed with {indexing_failures} failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'load_default_vocab_map',\n",
       " 'definition': \"def load_default_vocab_map():\\n    with open(Path(DEFAULT_VOCAB_MAP_FP) / 'vocab.json', 'r') as f:\\n        vocab_map = json.load(f)\\n    return vocab_map\",\n",
       " 'file_name': 'preprocessing.py',\n",
       " 'docstring': '',\n",
       " 'description': 'The `load_default_vocab_map()` function is responsible for loading a vocabulary map from a JSON file. It first opens the file located at `DEFAULT_VOCAB_MAP_FP/vocab.json` in read mode. Then, it uses the `json.load()` function to parse the JSON data from the file and store it in the `vocab_map` variable. Finally, the function returns the `vocab_map`, which is a dictionary containing the vocabulary mapping. This function is likely used in a natural language processing or machine learning application where a predefined vocabulary is required for processing text data. By loading the vocabulary map from a file, the application can ensure consistency and avoid hardcoding the vocabulary within the code. This approach also allows for easy modification of the vocabulary by simply updating the JSON file.\\n',\n",
       " 'uses': '',\n",
       " 'embeddings': [0.0044830255,\n",
       "  -0.007266335,\n",
       "  -0.012937576,\n",
       "  -0.006104956,\n",
       "  0.0016202049,\n",
       "  0.02054171,\n",
       "  0.036517456,\n",
       "  -0.04967847,\n",
       "  -0.008804722,\n",
       "  -0.010213871,\n",
       "  -0.0040165563,\n",
       "  -0.0037529315,\n",
       "  -0.022305451,\n",
       "  0.008779343,\n",
       "  0.009393459,\n",
       "  -0.06475662,\n",
       "  -0.023160787,\n",
       "  0.052209817,\n",
       "  -0.0141078215,\n",
       "  0.0039954805,\n",
       "  0.018289391,\n",
       "  -0.051774785,\n",
       "  0.028597184,\n",
       "  -0.016423656,\n",
       "  -0.040464927,\n",
       "  -0.014022977,\n",
       "  0.030819923,\n",
       "  -0.022280974,\n",
       "  0.04997819,\n",
       "  0.005752218,\n",
       "  0.017120646,\n",
       "  -0.037821215,\n",
       "  0.045860544,\n",
       "  -0.028777089,\n",
       "  -0.050940715,\n",
       "  0.028276034,\n",
       "  0.009457057,\n",
       "  -0.021420518,\n",
       "  0.019402571,\n",
       "  -0.055213243,\n",
       "  -0.06410745,\n",
       "  -0.028859725,\n",
       "  -0.05305581,\n",
       "  0.030116262,\n",
       "  -0.047906768,\n",
       "  0.023475071,\n",
       "  -0.014836683,\n",
       "  0.025077382,\n",
       "  -0.017006235,\n",
       "  0.016177334,\n",
       "  0.02951645,\n",
       "  -0.070366055,\n",
       "  -0.035339978,\n",
       "  0.059525967,\n",
       "  -0.019173097,\n",
       "  -0.010337982,\n",
       "  -0.027038477,\n",
       "  -0.049890738,\n",
       "  0.063695274,\n",
       "  -0.019777853,\n",
       "  -0.035003908,\n",
       "  -0.01600622,\n",
       "  0.023997454,\n",
       "  -0.031195471,\n",
       "  -0.003489773,\n",
       "  0.025318613,\n",
       "  -0.028395988,\n",
       "  -0.035607785,\n",
       "  0.03265622,\n",
       "  0.006288273,\n",
       "  -0.015748635,\n",
       "  0.08057346,\n",
       "  -0.05647237,\n",
       "  0.033325113,\n",
       "  -0.015733352,\n",
       "  0.028147181,\n",
       "  0.0037174392,\n",
       "  0.02405925,\n",
       "  0.01320987,\n",
       "  -0.010046457,\n",
       "  -0.059357148,\n",
       "  0.006467882,\n",
       "  0.0070426604,\n",
       "  0.01022212,\n",
       "  0.021898497,\n",
       "  -0.032824162,\n",
       "  -0.0048798104,\n",
       "  -0.004690435,\n",
       "  -0.030535826,\n",
       "  0.024625162,\n",
       "  0.059699163,\n",
       "  -0.008833492,\n",
       "  0.021044528,\n",
       "  -0.01273457,\n",
       "  0.09087022,\n",
       "  -0.028825315,\n",
       "  -0.044610962,\n",
       "  -0.1236465,\n",
       "  -0.012865343,\n",
       "  0.033147816,\n",
       "  0.026917176,\n",
       "  -0.019067166,\n",
       "  0.03575257,\n",
       "  -0.0019667533,\n",
       "  -0.011699128,\n",
       "  0.057332296,\n",
       "  -0.009828476,\n",
       "  -0.010214285,\n",
       "  -0.105759606,\n",
       "  0.008208618,\n",
       "  0.0034804542,\n",
       "  0.03796718,\n",
       "  -0.00012202938,\n",
       "  -0.036734574,\n",
       "  -0.017342623,\n",
       "  -0.0021508217,\n",
       "  0.017014805,\n",
       "  -0.0014246855,\n",
       "  -0.0070056147,\n",
       "  -0.010955814,\n",
       "  0.0023141,\n",
       "  0.05519215,\n",
       "  0.0012569629,\n",
       "  0.028827159,\n",
       "  0.038988266,\n",
       "  0.020336172,\n",
       "  0.0019382586,\n",
       "  -0.057008665,\n",
       "  -0.044520326,\n",
       "  -0.023080371,\n",
       "  0.08340818,\n",
       "  -0.027760863,\n",
       "  -0.026872287,\n",
       "  -0.012985898,\n",
       "  0.008426758,\n",
       "  -0.027318401,\n",
       "  -0.0037131642,\n",
       "  0.00944915,\n",
       "  -0.0008009883,\n",
       "  0.035119608,\n",
       "  -0.04068419,\n",
       "  -0.022890102,\n",
       "  -0.044848625,\n",
       "  0.02572188,\n",
       "  0.031642504,\n",
       "  0.006486433,\n",
       "  0.0062107113,\n",
       "  0.09231114,\n",
       "  0.011233209,\n",
       "  0.040161204,\n",
       "  -0.03672287,\n",
       "  -0.015351724,\n",
       "  -0.03022556,\n",
       "  0.067944564,\n",
       "  -0.030662894,\n",
       "  -0.01164667,\n",
       "  -0.015068494,\n",
       "  -0.035730164,\n",
       "  0.0039342386,\n",
       "  -0.013495354,\n",
       "  0.00016766248,\n",
       "  -0.007824204,\n",
       "  0.01887936,\n",
       "  0.03861447,\n",
       "  -0.024042176,\n",
       "  -0.053409085,\n",
       "  0.005049072,\n",
       "  -0.04248564,\n",
       "  0.032567404,\n",
       "  0.062409587,\n",
       "  -0.03215073,\n",
       "  0.015754951,\n",
       "  -0.06686261,\n",
       "  -0.038250744,\n",
       "  0.030838445,\n",
       "  -0.009423803,\n",
       "  0.0141296135,\n",
       "  -0.033083025,\n",
       "  -0.016420692,\n",
       "  -0.056685347,\n",
       "  0.11583508,\n",
       "  0.020196348,\n",
       "  -0.0035394873,\n",
       "  -0.08138201,\n",
       "  0.008872832,\n",
       "  -0.015230113,\n",
       "  -0.04950243,\n",
       "  0.020199068,\n",
       "  0.085762575,\n",
       "  0.07213335,\n",
       "  -0.0099900495,\n",
       "  -0.0057055596,\n",
       "  0.03085717,\n",
       "  -0.039586347,\n",
       "  -0.030064324,\n",
       "  -0.012514712,\n",
       "  0.06787473,\n",
       "  -0.067038134,\n",
       "  -0.01726501,\n",
       "  -0.0760207,\n",
       "  0.03184315,\n",
       "  -0.021802342,\n",
       "  0.0005503919,\n",
       "  -0.038563505,\n",
       "  -0.05812986,\n",
       "  -0.01806102,\n",
       "  0.0023229355,\n",
       "  -0.0062840073,\n",
       "  -0.017249268,\n",
       "  -0.022639774,\n",
       "  -0.028357431,\n",
       "  -0.048748948,\n",
       "  0.03903263,\n",
       "  -0.059752725,\n",
       "  -0.009508259,\n",
       "  0.023258373,\n",
       "  0.077454664,\n",
       "  0.021600474,\n",
       "  0.08439429,\n",
       "  0.019730458,\n",
       "  -0.02811022,\n",
       "  0.0023955295,\n",
       "  0.017418299,\n",
       "  -0.018554296,\n",
       "  0.016991094,\n",
       "  0.03526572,\n",
       "  -0.025197178,\n",
       "  0.043007284,\n",
       "  -0.001954447,\n",
       "  -0.042808466,\n",
       "  -0.005995364,\n",
       "  0.028541937,\n",
       "  -0.012374674,\n",
       "  0.03457727,\n",
       "  0.058707777,\n",
       "  0.042027105,\n",
       "  0.020759825,\n",
       "  -0.055644423,\n",
       "  -0.050834518,\n",
       "  0.022579348,\n",
       "  0.009797682,\n",
       "  0.02091259,\n",
       "  -0.007188385,\n",
       "  0.04684592,\n",
       "  0.065462805,\n",
       "  0.022193361,\n",
       "  0.02702316,\n",
       "  0.0052304915,\n",
       "  -0.015723826,\n",
       "  -0.013231246,\n",
       "  -0.021812348,\n",
       "  0.009194854,\n",
       "  -0.06063637,\n",
       "  -0.017781662,\n",
       "  -0.0478148,\n",
       "  -0.01931218,\n",
       "  -0.0106677925,\n",
       "  -0.03281316,\n",
       "  0.040102217,\n",
       "  0.030251123,\n",
       "  -0.0014603758,\n",
       "  -0.033736832,\n",
       "  -0.03399527,\n",
       "  -0.0478839,\n",
       "  -0.065131456,\n",
       "  -0.02541669,\n",
       "  -0.007353588,\n",
       "  -0.021167208,\n",
       "  0.018871514,\n",
       "  -0.051121365,\n",
       "  0.030773057,\n",
       "  0.020892723,\n",
       "  -0.005085304,\n",
       "  -0.015695142,\n",
       "  -0.04486947,\n",
       "  0.021327747,\n",
       "  -0.049903426,\n",
       "  0.056010246,\n",
       "  0.0009378833,\n",
       "  -0.053064108,\n",
       "  0.0037317274,\n",
       "  -0.033777766,\n",
       "  0.028717037,\n",
       "  -0.08216542,\n",
       "  0.01045607,\n",
       "  -0.015161494,\n",
       "  -0.015957993,\n",
       "  0.008641615,\n",
       "  -0.051773585,\n",
       "  -0.033467293,\n",
       "  0.015037697,\n",
       "  0.018190293,\n",
       "  -0.069438666,\n",
       "  -0.06108248,\n",
       "  0.022204103,\n",
       "  0.0622627,\n",
       "  0.050169453,\n",
       "  0.028896956,\n",
       "  0.0024844878,\n",
       "  0.007165882,\n",
       "  0.03494244,\n",
       "  0.03261979,\n",
       "  -0.031965412,\n",
       "  0.042798623,\n",
       "  -0.0117789805,\n",
       "  0.017257903,\n",
       "  -0.029563421,\n",
       "  -0.038652558,\n",
       "  -0.012450906,\n",
       "  0.044377543,\n",
       "  -0.003203036,\n",
       "  0.0073460857,\n",
       "  -0.03801614,\n",
       "  -0.027726974,\n",
       "  -0.031196227,\n",
       "  0.027428988,\n",
       "  -0.11148505,\n",
       "  0.0044515072,\n",
       "  -0.03508281,\n",
       "  -0.028302439,\n",
       "  0.054186724,\n",
       "  0.05112561,\n",
       "  -0.0018561278,\n",
       "  -0.0766122,\n",
       "  0.022934671,\n",
       "  -0.062258255,\n",
       "  0.026014376,\n",
       "  0.028813837,\n",
       "  0.008838603,\n",
       "  -0.056480013,\n",
       "  0.021880476,\n",
       "  0.03565139,\n",
       "  0.075744,\n",
       "  -0.0017432524,\n",
       "  0.024179598,\n",
       "  0.01642086,\n",
       "  -0.015491442,\n",
       "  0.043766048,\n",
       "  0.02811686,\n",
       "  -0.013876547,\n",
       "  0.014337764,\n",
       "  0.004950442,\n",
       "  0.07166837,\n",
       "  0.049631093,\n",
       "  0.030719062,\n",
       "  -0.023064256,\n",
       "  -0.019402862,\n",
       "  -0.012099657,\n",
       "  -0.014463915,\n",
       "  -0.012065374,\n",
       "  -0.030399362,\n",
       "  0.02896314,\n",
       "  0.019619552,\n",
       "  0.020562468,\n",
       "  -0.015892517,\n",
       "  0.04587821,\n",
       "  0.02200343,\n",
       "  -0.045712423,\n",
       "  0.047776707,\n",
       "  0.009348329,\n",
       "  0.0015036267,\n",
       "  0.0064665605,\n",
       "  -0.023935324,\n",
       "  0.07292331,\n",
       "  -0.027690377,\n",
       "  -0.022312349,\n",
       "  0.065699875,\n",
       "  0.032619014,\n",
       "  -0.04618727,\n",
       "  -0.006182236,\n",
       "  0.018705878,\n",
       "  0.028265582,\n",
       "  0.007579666,\n",
       "  0.032117393,\n",
       "  0.060783952,\n",
       "  -0.060211748,\n",
       "  -0.010567759,\n",
       "  -0.049498547,\n",
       "  -0.0039135576,\n",
       "  -0.038712468,\n",
       "  -0.002390299,\n",
       "  -0.04501948,\n",
       "  0.0074291155,\n",
       "  -0.016211761,\n",
       "  0.014183393,\n",
       "  -0.0061629424,\n",
       "  -0.0077441735,\n",
       "  0.0012388778,\n",
       "  0.047293875,\n",
       "  -0.07157438,\n",
       "  -0.019202339,\n",
       "  -0.0009143643,\n",
       "  0.040539913,\n",
       "  0.028283931,\n",
       "  0.021135386,\n",
       "  -0.018304722,\n",
       "  -0.036074907,\n",
       "  0.024803841,\n",
       "  0.008855389,\n",
       "  0.043652985,\n",
       "  0.014023419,\n",
       "  0.008051113,\n",
       "  0.010949188,\n",
       "  -0.06526361,\n",
       "  -0.010206514,\n",
       "  -0.00690263,\n",
       "  0.031879913,\n",
       "  0.014134308,\n",
       "  0.00039702913,\n",
       "  -0.056167156,\n",
       "  0.043143537,\n",
       "  0.039148297,\n",
       "  0.027348526,\n",
       "  0.081820585,\n",
       "  0.06533357,\n",
       "  0.019959135,\n",
       "  0.030017717,\n",
       "  0.009634722,\n",
       "  -0.015533739,\n",
       "  0.053648125,\n",
       "  0.040498197,\n",
       "  0.015236268,\n",
       "  -0.009931185,\n",
       "  0.012500721,\n",
       "  0.061765976,\n",
       "  -0.044357475,\n",
       "  0.022019897,\n",
       "  0.023620594,\n",
       "  0.0055591087,\n",
       "  0.026914464,\n",
       "  -0.026182849,\n",
       "  0.013337042,\n",
       "  -0.036599763,\n",
       "  0.012987173,\n",
       "  -0.019102683,\n",
       "  0.05462622,\n",
       "  -0.011284985,\n",
       "  -0.086117275,\n",
       "  0.001503914,\n",
       "  0.029235391,\n",
       "  0.050155636,\n",
       "  -0.00029778754,\n",
       "  0.038987152,\n",
       "  -0.0055453815,\n",
       "  -0.0035820662,\n",
       "  0.0105209965,\n",
       "  -0.0092785545,\n",
       "  -0.037859686,\n",
       "  0.005933816,\n",
       "  -0.046529062,\n",
       "  -0.014465649,\n",
       "  -0.028636044,\n",
       "  -0.09401941,\n",
       "  -0.09052151,\n",
       "  -0.10870456,\n",
       "  0.046230398,\n",
       "  0.013469428,\n",
       "  0.044350818,\n",
       "  -0.0062049306,\n",
       "  0.034098733,\n",
       "  -0.017477358,\n",
       "  -0.04194562,\n",
       "  0.042519767,\n",
       "  -0.004920543,\n",
       "  -0.018842625,\n",
       "  0.015343618,\n",
       "  -0.011698593,\n",
       "  0.020061601,\n",
       "  0.003865427,\n",
       "  0.011482768,\n",
       "  0.03731605,\n",
       "  0.0022161421,\n",
       "  0.01885823,\n",
       "  0.00033810097,\n",
       "  -0.028923046,\n",
       "  0.015081469,\n",
       "  0.03724166,\n",
       "  -0.012010068,\n",
       "  0.020746708,\n",
       "  -0.03979681,\n",
       "  -0.05869882,\n",
       "  0.038712826,\n",
       "  0.06438237,\n",
       "  0.069467425,\n",
       "  0.06832808,\n",
       "  0.009693404,\n",
       "  0.002987183,\n",
       "  0.03139202,\n",
       "  -0.038998857,\n",
       "  0.012775529,\n",
       "  -0.032369975,\n",
       "  0.025371637,\n",
       "  -0.030347226,\n",
       "  -0.009968424,\n",
       "  0.040505365,\n",
       "  0.0010494936,\n",
       "  0.058070775,\n",
       "  -0.029977964,\n",
       "  -0.04349799,\n",
       "  0.02387791,\n",
       "  0.08149866,\n",
       "  0.0405062,\n",
       "  -0.019726431,\n",
       "  -0.06869978,\n",
       "  0.0075091324,\n",
       "  0.01935477,\n",
       "  -0.02183689,\n",
       "  0.06313006,\n",
       "  0.04695277,\n",
       "  0.023218384,\n",
       "  0.043938357,\n",
       "  -0.03915988,\n",
       "  0.049164664,\n",
       "  0.03223497,\n",
       "  -0.043665227,\n",
       "  0.03145466,\n",
       "  -0.017252479,\n",
       "  0.0025963923,\n",
       "  -0.0634479,\n",
       "  0.0607815,\n",
       "  -0.02367406,\n",
       "  0.018102342,\n",
       "  0.0012695713,\n",
       "  -0.034410536,\n",
       "  -0.04758201,\n",
       "  -0.016141253,\n",
       "  -0.060992856,\n",
       "  0.045934673,\n",
       "  -0.003911174,\n",
       "  -0.026390782,\n",
       "  0.0038801846,\n",
       "  0.0465955,\n",
       "  -0.0037907122,\n",
       "  -0.01966156,\n",
       "  -0.015544022,\n",
       "  0.09889734,\n",
       "  0.051381014,\n",
       "  0.03548786,\n",
       "  0.01112292,\n",
       "  0.010635545,\n",
       "  0.00021804933,\n",
       "  -0.010593745,\n",
       "  -0.040947318,\n",
       "  0.017212765,\n",
       "  0.0044350983,\n",
       "  -0.045564797,\n",
       "  0.02312275,\n",
       "  0.0534448,\n",
       "  -0.0029366273,\n",
       "  0.049382523,\n",
       "  -0.01950947,\n",
       "  -0.036100913,\n",
       "  -0.032407776,\n",
       "  0.038991902,\n",
       "  -0.023414824,\n",
       "  -0.040947612,\n",
       "  -0.008840035,\n",
       "  -0.022167886,\n",
       "  0.012955781,\n",
       "  -0.03265807,\n",
       "  0.0068129576,\n",
       "  0.005136394,\n",
       "  -0.010436652,\n",
       "  -0.021896621,\n",
       "  -0.027781611,\n",
       "  0.011338205,\n",
       "  -0.041288357,\n",
       "  -0.0075980336,\n",
       "  -0.043677032,\n",
       "  -0.004841429,\n",
       "  0.006086489,\n",
       "  -0.046648487,\n",
       "  -0.009218796,\n",
       "  -0.029811861,\n",
       "  0.0057272348,\n",
       "  -0.019616641,\n",
       "  -0.0812926,\n",
       "  0.006965919,\n",
       "  0.006058981,\n",
       "  -0.0040319664,\n",
       "  -0.0059005762,\n",
       "  -0.044943627,\n",
       "  0.057162542,\n",
       "  -0.006369117,\n",
       "  0.0030922007,\n",
       "  0.008796437,\n",
       "  0.021602022,\n",
       "  0.014015156,\n",
       "  -0.004077762,\n",
       "  -0.031853408,\n",
       "  -0.003973578,\n",
       "  0.03520515,\n",
       "  -0.009074383,\n",
       "  -0.053683992,\n",
       "  0.014528519,\n",
       "  -0.06974916,\n",
       "  -0.049813665,\n",
       "  -0.015040594,\n",
       "  0.037296657,\n",
       "  -0.051680893,\n",
       "  0.011462632,\n",
       "  -0.048784226,\n",
       "  0.05013123,\n",
       "  0.04113927,\n",
       "  -0.0054447553,\n",
       "  -0.04293352,\n",
       "  -0.05306194,\n",
       "  0.0041137477,\n",
       "  -0.021327438,\n",
       "  0.078834444,\n",
       "  0.024981901,\n",
       "  0.018688733,\n",
       "  -0.04386203,\n",
       "  -0.025439376,\n",
       "  -0.039527837,\n",
       "  0.07686246,\n",
       "  -0.040019613,\n",
       "  -0.011673661,\n",
       "  0.004274992,\n",
       "  0.046251103,\n",
       "  0.0027702264,\n",
       "  -0.0076705758,\n",
       "  -0.003688111,\n",
       "  -0.027094036,\n",
       "  -0.07440164,\n",
       "  -0.016894896,\n",
       "  -0.003153009,\n",
       "  -0.035071183,\n",
       "  0.08233901,\n",
       "  -0.0126526095,\n",
       "  -0.021874284,\n",
       "  -0.01754742,\n",
       "  0.09097476,\n",
       "  -0.051256284,\n",
       "  -0.016892701,\n",
       "  -0.034557972,\n",
       "  0.042454675,\n",
       "  0.0014715452,\n",
       "  0.029762108,\n",
       "  -0.039177675,\n",
       "  -0.0119023,\n",
       "  -0.011719133,\n",
       "  0.018098602,\n",
       "  -0.011109628,\n",
       "  -0.025724811,\n",
       "  -0.008171264,\n",
       "  -0.048983034,\n",
       "  -0.000543363,\n",
       "  0.02242593,\n",
       "  0.028408343,\n",
       "  -0.025976349,\n",
       "  0.0027362974,\n",
       "  0.008348208,\n",
       "  0.030538488,\n",
       "  -0.068862736,\n",
       "  -0.027471907,\n",
       "  -0.026910972,\n",
       "  -0.014535503,\n",
       "  0.03738569,\n",
       "  -0.034821104,\n",
       "  0.023146523,\n",
       "  0.014162087,\n",
       "  -0.033821,\n",
       "  0.015247133,\n",
       "  -0.016383559,\n",
       "  -0.024720097,\n",
       "  -0.026006399,\n",
       "  -0.007246287,\n",
       "  0.009766922,\n",
       "  0.0012609526,\n",
       "  -0.00049581385,\n",
       "  0.066931404,\n",
       "  -0.022555549,\n",
       "  0.07581436,\n",
       "  -0.058554523,\n",
       "  -0.0066056903,\n",
       "  0.013957587,\n",
       "  -0.010795092,\n",
       "  -0.013242851,\n",
       "  0.045629993,\n",
       "  -0.004505102,\n",
       "  -0.034792956,\n",
       "  0.01017548,\n",
       "  0.025283456,\n",
       "  0.020990707,\n",
       "  0.018237803,\n",
       "  0.05553557,\n",
       "  -0.011287659,\n",
       "  -0.035366833,\n",
       "  -0.0077239834,\n",
       "  -0.015902622,\n",
       "  -0.010400792,\n",
       "  -0.0016546929,\n",
       "  0.056804765,\n",
       "  -0.035330743,\n",
       "  0.07156775,\n",
       "  0.0140291015,\n",
       "  0.039753318,\n",
       "  -0.082020015,\n",
       "  -0.021010065,\n",
       "  0.025123656,\n",
       "  -0.03698623,\n",
       "  -0.04108119,\n",
       "  -0.027219605,\n",
       "  0.057374425,\n",
       "  -0.000983025,\n",
       "  0.06334059,\n",
       "  0.032364868,\n",
       "  -0.044843297,\n",
       "  -0.003954305,\n",
       "  -0.031284116,\n",
       "  0.008095426,\n",
       "  0.016565183,\n",
       "  0.06579996,\n",
       "  0.021889463,\n",
       "  0.03944875,\n",
       "  0.016137194,\n",
       "  0.05014821,\n",
       "  0.066349395,\n",
       "  -0.019958096,\n",
       "  0.036382746,\n",
       "  -0.069793,\n",
       "  0.021033522,\n",
       "  0.029505555,\n",
       "  -0.035914242,\n",
       "  0.04508037,\n",
       "  -0.010923483,\n",
       "  0.020236226,\n",
       "  -0.003927141,\n",
       "  -0.014306729,\n",
       "  0.024790712,\n",
       "  0.025379866,\n",
       "  -0.031627804,\n",
       "  0.09068121,\n",
       "  -0.027061291,\n",
       "  -0.013419166,\n",
       "  0.0099244835,\n",
       "  0.019027073,\n",
       "  9.955182e-05,\n",
       "  -0.00964572,\n",
       "  0.010148476,\n",
       "  0.02029679,\n",
       "  -0.06397592,\n",
       "  -0.040181782,\n",
       "  0.011085793,\n",
       "  0.0089825345,\n",
       "  0.01460123,\n",
       "  -0.008393565,\n",
       "  -0.021851841,\n",
       "  -0.04134575,\n",
       "  -0.04047285,\n",
       "  -0.016844345,\n",
       "  0.019024277,\n",
       "  -0.02079658,\n",
       "  -0.023137186,\n",
       "  0.03636376,\n",
       "  -0.0051171007,\n",
       "  0.012608497,\n",
       "  -0.031782877,\n",
       "  -0.011993995,\n",
       "  -0.027818505,\n",
       "  0.016486557,\n",
       "  0.014013358,\n",
       "  0.01563092,\n",
       "  -0.052991327,\n",
       "  -0.0703438,\n",
       "  0.049032237,\n",
       "  -0.003363073]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.json().get(\"code:010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Tokenizer takes lot time to complete train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_queries = get_embeddings(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_query_with_range = (\n",
    "    Query(\"@vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: score}\")\n",
    "    .sort_by('score')\n",
    "    .return_fields('score', 'id', 'name', 'definition', 'file_name', 'type', 'uses')\n",
    "    .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_query = (\n",
    "    Query('*=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "    .sort_by('vector_score')\n",
    "    .return_fields('vector_score', 'id', 'name', 'definition', 'file_name', 'type', 'uses')\n",
    "    .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document {'id': 'code:002', 'payload': None, 'vector_score': '0.402300059795', 'name': 'Tokenizer', 'definition': 'class Tokenizer:\\n\\n    def __init__(self, max_length: int, unknown_token: str=\\'<unk>\\', pad_token: str=\\'<pad>\\'):\\n        self.unknown_token_id = None\\n        self.token_id_to_token_map = None\\n        self.vocab_map = None\\n        self.unknown_token = unknown_token\\n        self.pad_token = pad_token\\n        self.tokenized_chars = []\\n        self.unique_chars = []\\n        self.special_tokens = [self.unknown_token, self.pad_token]\\n        self.max_length = max_length\\n        self.pad_token_id = None\\n\\n    def __encode(self, text, truncate_and_pad: bool) -> list:\\n        processed_text = self.__process_text(text)\\n        text_encodings = [self.vocab_map.get(char, self.unknown_token_id) for char in processed_text]\\n        if truncate_and_pad:\\n            return self.pad_or_truncate(sequence=text_encodings, max_length=self.max_length, padding_value=self.pad_token_id)\\n        else:\\n            return text_encodings\\n\\n    @staticmethod\\n    def pad_or_truncate(sequence, max_length, padding_value):\\n        if len(sequence) > max_length:\\n            return sequence[:max_length]\\n        elif len(sequence) < max_length:\\n            return sequence + [padding_value] * (max_length - len(sequence))\\n        else:\\n            return sequence\\n\\n    def __call__(self, text, truncate_and_pad: bool=True) -> list:\\n        \"\"\"\\n        Encode the given text into a list of tokens.\\n\\n        Parameters\\n        ----------\\n        text : str\\n            Text to be encoded.\\n        truncate_and_pad: bool\\n            Set as True if you need to truncate/pad encodings False otherwise\\n\\n        Returns\\n        -------\\n        encoded_tokens : list of int\\n            List of tokens representing the encoded text.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> corpus = [...]\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train(corpus)\\n        >>> tokenizer(\"මම ගෙදර ගියා\")\\n        [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\\n        \"\"\"\\n        return self.__encode(text, truncate_and_pad=truncate_and_pad)\\n\\n    def decode(self, ids, skip_special_tokens: bool=False) -> str:\\n        \"\"\"\\n        Decode a list of token IDs into a string.\\n\\n        Parameters\\n        ----------\\n        ids : list of int\\n            List of token IDs to be decoded.\\n        skip_special_tokens: bool\\n            Whether to consider special tokens when decoding sequences\\n\\n        Returns\\n        -------\\n        decoded_text : str\\n            The decoded text string.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train([...])\\n        >>> encoded_tokens = [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\\n        >>> tokenizer.decode(encoded_tokens)\\n        \\'මම ගෙදර ගියා\\'\\n        \"\"\"\\n        special_token_ids = [self.vocab_map[tok] for tok in self.special_tokens]\\n        if skip_special_tokens:\\n            return \\'\\'.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids if token not in special_token_ids])\\n        else:\\n            return \\'\\'.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids])\\n\\n    def train(self, text_list) -> None:\\n        \"\"\"\\n        Train the tokenizer on a list of text strings.\\n\\n        Parameters\\n        ----------\\n        text_list : list of str\\n            List of text strings to be used for training the tokenizer.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> corpus = [...]\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train(corpus)\\n        \"\"\"\\n        self.__train_character_level_tokenizer(text_list)\\n\\n    def __len__(self):\\n        return len(self.vocab_map)\\n\\n    @property\\n    def vocab_size(self):\\n        return len(self)\\n\\n    @staticmethod\\n    def __process_text(t):\\n        return process_text(t)\\n\\n    def __train_character_level_tokenizer(self, text_list):\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            results = list(executor.map(self.__process_text, text_list))\\n            self.tokenized_chars = [char for sublist in results for char in sublist]\\n        self.unique_chars = set(self.tokenized_chars)\\n        self.vocab_map = dict(zip(self.unique_chars, range(len(self.unique_chars))))\\n        self.vocab_map[self.unknown_token] = len(self.vocab_map)\\n        self.vocab_map[self.pad_token] = len(self.vocab_map)\\n        self.unknown_token_id = self.vocab_map[self.unknown_token]\\n        self.pad_token_id = self.vocab_map[self.pad_token]\\n        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\\n\\n    def load_from_pretrained(self, file_path: str) -> None:\\n        \"\"\"\\n        Load the vocabulary map from a pre-trained file.\\n\\n        Parameters\\n        ----------\\n        file_path : str\\n            Path to the file containing the pre-trained vocabulary map.\\n\\n        Returns\\n        -------\\n        None\\n\\n        Warns\\n        -----\\n        UserWarning\\n            If the file is not found at the specified path, a default vocabulary map is loaded and a warning is issued.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.load_from_pretrained(\"pretrained_vocab.json\")\\n        \"\"\"\\n        file_path = Path(file_path)\\n        if file_path.exists():\\n            with open(file_path / \\'vocab.json\\', \\'r\\') as f:\\n                self.vocab_map = json.load(f)\\n            with open(file_path / \\'config.json\\', \\'r\\') as f:\\n                configurations = json.load(f)\\n            self.unknown_token = configurations[\\'unknown_token\\']\\n            self.pad_token = configurations[\\'pad_token\\']\\n            self.unknown_token_id = configurations[\\'unknown_token_id\\']\\n            self.pad_token_id = configurations[\\'pad_token_id\\']\\n            self.max_length = configurations[\\'max_length\\']\\n        else:\\n            warnings.warn(\\'File not found at the specified path. Loaded default vocab map.\\', UserWarning)\\n            self.vocab_map = load_default_vocab_map()\\n        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\\n        self.unknown_token_id = self.vocab_map[self.unknown_token]\\n        self.pad_token_id = self.vocab_map[self.pad_token]\\n        return self\\n\\n    def save_tokenizer(self, save_path: str):\\n        save_path = Path(save_path)\\n        configurations = {\\'unknown_token\\': self.unknown_token, \\'pad_token\\': self.pad_token, \\'unknown_token_id\\': self.unknown_token_id, \\'pad_token_id\\': self.pad_token_id, \\'max_length\\': self.max_length}\\n        with open(save_path / \\'vocab.json\\', \\'w\\', encoding=\\'utf-8\\') as file:\\n            json.dump(self.vocab_map, file, ensure_ascii=False, indent=4)\\n        with open(save_path / \\'config.json\\', \\'w\\') as file:\\n            json.dump(configurations, file, indent=4)', 'file_name': 'tokenizer.py', 'type': 'class', 'uses': 'process_text, load_default_vocab_map'},\n",
       " Document {'id': 'code:004', 'payload': None, 'vector_score': '0.467481791973', 'name': 'load_tokenizer', 'definition': 'def load_tokenizer():\\n    tokenizer = Tokenizer(max_length=MAX_LENGTH)\\n    tokenizer.load_from_pretrained(DUMMY_FILE_NAME)\\n    return tokenizer', 'file_name': 'dataset_utils.py', 'type': 'function', 'uses': 'Tokenizer, process_text, load_default_vocab_map'},\n",
       " Document {'id': 'code:006', 'payload': None, 'vector_score': '0.49214309454', 'name': 'load_transliterator_model', 'definition': 'def load_transliterator_model():\\n    tokenizer = load_tokenizer()\\n    input_size = len(tokenizer)\\n    output_size = len(tokenizer)\\n    hidden_size = HIDDEN_SIZE\\n    filepath = Path(MODELS_PATH) / CHECKPOINT_NAME\\n    device = detect_device()\\n    model = BiLSTMTranslator(input_size, hidden_size, output_size).to(device)\\n    checkpoint = torch.load(filepath, map_location=device)\\n    model.load_state_dict(checkpoint)\\n    return model', 'file_name': 'model_utils.py', 'type': 'function', 'uses': 'Tokenizer, detect_device, load_default_vocab_map, process_text, BiLSTMTranslator, load_tokenizer'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ft(INDEX_NAME).search(\n",
    "    vector_search_query,\n",
    "    {\n",
    "      'query_vector': np.array(encoded_queries[0], dtype=np.float32).tobytes()\n",
    "    }\n",
    ").docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document {'id': 'code:002', 'payload': None, 'score': '0.402300059795', 'name': 'Tokenizer', 'definition': 'class Tokenizer:\\n\\n    def __init__(self, max_length: int, unknown_token: str=\\'<unk>\\', pad_token: str=\\'<pad>\\'):\\n        self.unknown_token_id = None\\n        self.token_id_to_token_map = None\\n        self.vocab_map = None\\n        self.unknown_token = unknown_token\\n        self.pad_token = pad_token\\n        self.tokenized_chars = []\\n        self.unique_chars = []\\n        self.special_tokens = [self.unknown_token, self.pad_token]\\n        self.max_length = max_length\\n        self.pad_token_id = None\\n\\n    def __encode(self, text, truncate_and_pad: bool) -> list:\\n        processed_text = self.__process_text(text)\\n        text_encodings = [self.vocab_map.get(char, self.unknown_token_id) for char in processed_text]\\n        if truncate_and_pad:\\n            return self.pad_or_truncate(sequence=text_encodings, max_length=self.max_length, padding_value=self.pad_token_id)\\n        else:\\n            return text_encodings\\n\\n    @staticmethod\\n    def pad_or_truncate(sequence, max_length, padding_value):\\n        if len(sequence) > max_length:\\n            return sequence[:max_length]\\n        elif len(sequence) < max_length:\\n            return sequence + [padding_value] * (max_length - len(sequence))\\n        else:\\n            return sequence\\n\\n    def __call__(self, text, truncate_and_pad: bool=True) -> list:\\n        \"\"\"\\n        Encode the given text into a list of tokens.\\n\\n        Parameters\\n        ----------\\n        text : str\\n            Text to be encoded.\\n        truncate_and_pad: bool\\n            Set as True if you need to truncate/pad encodings False otherwise\\n\\n        Returns\\n        -------\\n        encoded_tokens : list of int\\n            List of tokens representing the encoded text.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> corpus = [...]\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train(corpus)\\n        >>> tokenizer(\"මම ගෙදර ගියා\")\\n        [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\\n        \"\"\"\\n        return self.__encode(text, truncate_and_pad=truncate_and_pad)\\n\\n    def decode(self, ids, skip_special_tokens: bool=False) -> str:\\n        \"\"\"\\n        Decode a list of token IDs into a string.\\n\\n        Parameters\\n        ----------\\n        ids : list of int\\n            List of token IDs to be decoded.\\n        skip_special_tokens: bool\\n            Whether to consider special tokens when decoding sequences\\n\\n        Returns\\n        -------\\n        decoded_text : str\\n            The decoded text string.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train([...])\\n        >>> encoded_tokens = [2041, 2041, 942, 965, 624, 909, 942, 54, 1960]\\n        >>> tokenizer.decode(encoded_tokens)\\n        \\'මම ගෙදර ගියා\\'\\n        \"\"\"\\n        special_token_ids = [self.vocab_map[tok] for tok in self.special_tokens]\\n        if skip_special_tokens:\\n            return \\'\\'.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids if token not in special_token_ids])\\n        else:\\n            return \\'\\'.join([self.token_id_to_token_map.get(token, self.unknown_token) for token in ids])\\n\\n    def train(self, text_list) -> None:\\n        \"\"\"\\n        Train the tokenizer on a list of text strings.\\n\\n        Parameters\\n        ----------\\n        text_list : list of str\\n            List of text strings to be used for training the tokenizer.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> corpus = [...]\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.train(corpus)\\n        \"\"\"\\n        self.__train_character_level_tokenizer(text_list)\\n\\n    def __len__(self):\\n        return len(self.vocab_map)\\n\\n    @property\\n    def vocab_size(self):\\n        return len(self)\\n\\n    @staticmethod\\n    def __process_text(t):\\n        return process_text(t)\\n\\n    def __train_character_level_tokenizer(self, text_list):\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            results = list(executor.map(self.__process_text, text_list))\\n            self.tokenized_chars = [char for sublist in results for char in sublist]\\n        self.unique_chars = set(self.tokenized_chars)\\n        self.vocab_map = dict(zip(self.unique_chars, range(len(self.unique_chars))))\\n        self.vocab_map[self.unknown_token] = len(self.vocab_map)\\n        self.vocab_map[self.pad_token] = len(self.vocab_map)\\n        self.unknown_token_id = self.vocab_map[self.unknown_token]\\n        self.pad_token_id = self.vocab_map[self.pad_token]\\n        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\\n\\n    def load_from_pretrained(self, file_path: str) -> None:\\n        \"\"\"\\n        Load the vocabulary map from a pre-trained file.\\n\\n        Parameters\\n        ----------\\n        file_path : str\\n            Path to the file containing the pre-trained vocabulary map.\\n\\n        Returns\\n        -------\\n        None\\n\\n        Warns\\n        -----\\n        UserWarning\\n            If the file is not found at the specified path, a default vocabulary map is loaded and a warning is issued.\\n\\n        Examples\\n        --------\\n        >>> from sinlib import Tokenizer\\n        >>> tokenizer = Tokenizer()\\n        >>> tokenizer.load_from_pretrained(\"pretrained_vocab.json\")\\n        \"\"\"\\n        file_path = Path(file_path)\\n        if file_path.exists():\\n            with open(file_path / \\'vocab.json\\', \\'r\\') as f:\\n                self.vocab_map = json.load(f)\\n            with open(file_path / \\'config.json\\', \\'r\\') as f:\\n                configurations = json.load(f)\\n            self.unknown_token = configurations[\\'unknown_token\\']\\n            self.pad_token = configurations[\\'pad_token\\']\\n            self.unknown_token_id = configurations[\\'unknown_token_id\\']\\n            self.pad_token_id = configurations[\\'pad_token_id\\']\\n            self.max_length = configurations[\\'max_length\\']\\n        else:\\n            warnings.warn(\\'File not found at the specified path. Loaded default vocab map.\\', UserWarning)\\n            self.vocab_map = load_default_vocab_map()\\n        self.token_id_to_token_map = {value: key for key, value in self.vocab_map.items()}\\n        self.unknown_token_id = self.vocab_map[self.unknown_token]\\n        self.pad_token_id = self.vocab_map[self.pad_token]\\n        return self\\n\\n    def save_tokenizer(self, save_path: str):\\n        save_path = Path(save_path)\\n        configurations = {\\'unknown_token\\': self.unknown_token, \\'pad_token\\': self.pad_token, \\'unknown_token_id\\': self.unknown_token_id, \\'pad_token_id\\': self.pad_token_id, \\'max_length\\': self.max_length}\\n        with open(save_path / \\'vocab.json\\', \\'w\\', encoding=\\'utf-8\\') as file:\\n            json.dump(self.vocab_map, file, ensure_ascii=False, indent=4)\\n        with open(save_path / \\'config.json\\', \\'w\\') as file:\\n            json.dump(configurations, file, indent=4)', 'file_name': 'tokenizer.py', 'type': 'class', 'uses': 'process_text, load_default_vocab_map'},\n",
       " Document {'id': 'code:004', 'payload': None, 'score': '0.467481791973', 'name': 'load_tokenizer', 'definition': 'def load_tokenizer():\\n    tokenizer = Tokenizer(max_length=MAX_LENGTH)\\n    tokenizer.load_from_pretrained(DUMMY_FILE_NAME)\\n    return tokenizer', 'file_name': 'dataset_utils.py', 'type': 'function', 'uses': 'Tokenizer, process_text, load_default_vocab_map'},\n",
       " Document {'id': 'code:006', 'payload': None, 'score': '0.49214309454', 'name': 'load_transliterator_model', 'definition': 'def load_transliterator_model():\\n    tokenizer = load_tokenizer()\\n    input_size = len(tokenizer)\\n    output_size = len(tokenizer)\\n    hidden_size = HIDDEN_SIZE\\n    filepath = Path(MODELS_PATH) / CHECKPOINT_NAME\\n    device = detect_device()\\n    model = BiLSTMTranslator(input_size, hidden_size, output_size).to(device)\\n    checkpoint = torch.load(filepath, map_location=device)\\n    model.load_state_dict(checkpoint)\\n    return model', 'file_name': 'model_utils.py', 'type': 'function', 'uses': 'Tokenizer, detect_device, load_default_vocab_map, process_text, BiLSTMTranslator, load_tokenizer'},\n",
       " Document {'id': 'code:007', 'payload': None, 'score': '0.499140918255', 'name': 'inference', 'definition': 'def inference(model, tokenizer, input_text):\\n    model.eval()\\n    device = detect_device()\\n    tokens_to_ignore = [tokenizer.vocab_map[tok] for tok in tokenizer.special_tokens]\\n    input_encoded = tokenizer(input_text)\\n    input_tensor = torch.tensor(input_encoded).unsqueeze(0).to(device)\\n    with torch.no_grad():\\n        output = model(input_tensor)\\n        predicted = output.argmax(dim=-1)\\n        pred = [p for p in predicted[0].tolist() if p not in tokens_to_ignore]\\n        translated_text = tokenizer.decode(pred)\\n    return translated_text', 'file_name': 'model_utils.py', 'type': 'function', 'uses': 'detect_device'},\n",
       " Document {'id': 'code:003', 'payload': None, 'score': '0.537450194359', 'name': 'Transliterator', 'definition': \"class Transliterator:\\n\\n    def __init__(self):\\n        self.model = load_transliterator_model()\\n        self.tokenizer = load_tokenizer()\\n\\n    def transliterate(self, text):\\n        word_list = text.split()\\n        transliterated_text = [inference(self.model, self.tokenizer, word) for word in word_list]\\n        return ' '.join(transliterated_text).strip()\", 'file_name': 'transliterate.py', 'type': 'class', 'uses': 'Tokenizer, detect_device, inference, load_default_vocab_map, load_transliterator_model, process_text, BiLSTMTranslator, load_tokenizer'},\n",
       " Document {'id': 'code:013', 'payload': None, 'score': '0.540332198143', 'name': 'process_text', 'definition': \"def process_text(t):\\n    tokenized_chars = []\\n    for i, char in enumerate(t):\\n        if char in VOWEL_DIACRITICS:\\n            continue\\n        if char in NUBERS_AND_PUNKTS:\\n            tokenized_chars.append(char)\\n        elif char == ' ':\\n            tokenized_chars.append(' ')\\n        elif char in ALL_LETTERS:\\n            if i < len(t) - 1 and t[i + 1] in ALL_LETTERS:\\n                tokenized_chars.append(char)\\n            elif i < len(t) - 1 and t[i + 1] in VOWEL_DIACRITICS:\\n                tokenized_chars.append(char + t[i + 1])\\n            else:\\n                tokenized_chars.append(char)\\n        else:\\n            tokenized_chars.append(char)\\n    return tokenized_chars\", 'file_name': 'preprocessing.py', 'type': 'function', 'uses': ''},\n",
       " Document {'id': 'code:001', 'payload': None, 'score': '0.551093459129', 'name': 'Romanizer', 'definition': \"class Romanizer:\\n\\n    def __init__(self, char_mapper_fp: str, tokenizer_path: str):\\n        if char_mapper_fp is None:\\n            char_mapper_fp = CHAR_MAPPER_FP\\n        if tokenizer_path is None:\\n            tokenizer_path = DEFAULT_VOCAB_MAP_FP\\n        self.char_mapper = load_char_mapper(char_mapper_fp)\\n        self.tokenizer = Tokenizer(max_length=None)\\n        self.tokenizer.load_from_pretrained(tokenizer_path)\\n\\n    def __call__(self, text):\\n        return self.__romanize(text)\\n\\n    def __romanize(self, text: str):\\n        text = remove_non_printable(text)\\n        chars = np.array(list(text))\\n        sinhala_mask = [True if ch in ALL_SINHALA_CHARACTERS + list(NUBERS_AND_PUNKTS) + [' '] else False for ch in chars]\\n        sinhala_text = ''.join(chars[sinhala_mask]).strip()\\n        encodings = self.tokenizer(sinhala_text, truncate_and_pad=False)\\n        decoded_sinhala_chars = [self.tokenizer.token_id_to_token_map[c] for c in encodings]\\n        romanized_sinhala = [self.char_mapper.get(ch, ch if ch in NUBERS_AND_PUNKTS.union(' ') else '') for ch in decoded_sinhala_chars]\\n        romanized_sinhala = ''.join(romanized_sinhala)\\n        word_2_word_mapping = dict(zip(sinhala_text.split(), romanized_sinhala.split()))\\n        romanized_text = [word_2_word_mapping.get(word, word) for word in text.split()]\\n        return ' '.join(romanized_text)\", 'file_name': 'romanize.py', 'type': 'class', 'uses': 'Tokenizer, load_default_vocab_map, load_char_mapper, remove_non_printable, process_text'},\n",
       " Document {'id': 'code:008', 'payload': None, 'score': '0.556816935539', 'name': 'BiLSTMTranslator', 'definition': 'class BiLSTMTranslator(nn.Module):\\n\\n    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\\n        super(BiLSTMTranslator, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.embedding = nn.Embedding(input_size, hidden_size)\\n        self.bilstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\\n        self.dropout = nn.Dropout(p=0.3)\\n        self.fc = nn.Linear(hidden_size * 2, output_size)\\n        self.log_softmax = nn.LogSoftmax(dim=-1)\\n\\n    def forward(self, x):\\n        embedded = self.embedding(x.long())\\n        out, _ = self.bilstm(embedded)\\n        out = self.layer_norm(out)\\n        out = self.dropout(out)\\n        out = self.fc(out)\\n        out = self.log_softmax(out)\\n        return out\\n\\n    def n_parameters(self):\\n        return sum((p.numel() for p in self.parameters() if p.requires_grad))', 'file_name': 'transliterator_model.py', 'type': 'class', 'uses': 'BiLSTMTranslator'},\n",
       " Document {'id': 'code:014', 'payload': None, 'score': '0.563104152679', 'name': 'process_text_with_token_counts', 'definition': 'def process_text_with_token_counts(t: str, consider_special_character_as_sinhala: bool, ignore_non_printable: bool):\\n    \"\"\"\\n    Process the given text, tokenizing it and counting the tokens.\\n\\n    Parameters\\n    ----------\\n    t : str\\n        The text to be processed.\\n    consider_special_character_as_sinhala : bool\\n        If True, special characters will be considered as Sinhala characters.\\n    ignore_non_printable : bool\\n        If True, non-printable characters will be removed from the text.\\n\\n    Returns\\n    -------\\n    tokenized_chars : list of str\\n        List of tokenized characters from the text.\\n    token_counts : int\\n        Total count of tokens in the text.\\n\\n    Examples\\n    --------\\n    >>> from sinlib.utils.preprocessing import process_text_with_token_counts\\n    >>> text = \"මම ගෙදර ගියා.\"\\n    >>> tokenized_chars, token_counts = process_text_with_token_counts(text, True, True)\\n    >>> print(tokenized_chars)\\n    [\\'ම\\', \\'ම\\', \\' \\', \\'ගෙ\\', \\'ද\\', \\'ර\\', \\' \\', \\'ගි\\', \\'යා\\', \\'.\\']\\n    >>> print(token_counts)\\n    10\\n    \"\"\"\\n    if ignore_non_printable:\\n        t = remove_non_printable(t)\\n    tokenized_chars = []\\n    token_counts = 0\\n    for i, char in enumerate(t):\\n        if char in VOWEL_DIACRITICS:\\n            continue\\n        if char in NUBERS_AND_PUNKTS and consider_special_character_as_sinhala:\\n            tokenized_chars.append(char)\\n            token_counts += 1\\n        elif char == \\' \\':\\n            tokenized_chars.append(\\' \\')\\n        elif char in ALL_LETTERS:\\n            token_counts += 1\\n            if i < len(t) - 1 and t[i + 1] in ALL_LETTERS:\\n                tokenized_chars.append(char)\\n            elif i < len(t) - 1 and t[i + 1] in VOWEL_DIACRITICS:\\n                tokenized_chars.append(char + t[i + 1])\\n            else:\\n                tokenized_chars.append(char)\\n        else:\\n            tokenized_chars.append(char)\\n    return (tokenized_chars, token_counts)', 'file_name': 'preprocessing.py', 'type': 'function', 'uses': 'remove_non_printable'},\n",
       " Document {'id': 'code:015', 'payload': None, 'score': '0.603789567947', 'name': 'get_sinhala_character_ratio', 'definition': 'def get_sinhala_character_ratio(text, consider_special_character_as_sinhala: bool=True, ignore_non_printable: bool=True):\\n    \"\"\"\\n    Calculate the ratio of Sinhala characters in the given text.\\n\\n    Parameters\\n    ----------\\n    text : str or list of str\\n        The text or list of text strings to be processed.\\n    consider_special_character_as_sinhala : bool, default=True\\n        If True, numbers and special characters will be considered as Sinhala characters.\\n    ignore_non_printable : bool, default=True\\n        If True, non-printable characters will be removed before processing.\\n\\n    Returns\\n    -------\\n    ratio : float or list of float\\n        The ratio of Sinhala characters in the text. If the input is a list, returns a list of ratios for each text string.\\n\\n    Examples\\n    --------\\n    >>> from sinlib.utils.preprocessing import get_sinhala_character_ratio\\n    >>> text = \"මම ගෙදර ගියා.\"\\n    >>> ratio = get_sinhala_character_ratio(text, True, True)\\n    >>> print(ratio)\\n    1.0\\n\\n    >>> texts = [\"මම ගෙදර ගියා.\", \"This is an example.\"]\\n    >>> ratio = get_sinhala_character_ratio(texts, False, True)\\n    >>> print(ratios)\\n    [0.875, 0.0]\\n    \"\"\"\\n    if isinstance(text, str):\\n        tokenized_text, sinhala_token_count = process_text_with_token_counts(text, consider_special_character_as_sinhala, ignore_non_printable=ignore_non_printable)\\n        tokenized_text = [tok for tok in tokenized_text if tok != \\' \\']\\n        return sinhala_token_count / len(tokenized_text)\\n    elif isinstance(text, list):\\n        pool = multiprocessing.Pool()\\n        partial_process_text = partial(process_text_with_token_counts, consider_special_character_as_sinhala=consider_special_character_as_sinhala, ignore_non_printable=ignore_non_printable)\\n        results = pool.map(partial_process_text, text)\\n        pool.close()\\n        pool.join()\\n        encodings = [tok[0] for tok in results if tok[0] != \\' \\']\\n        encodings = [[char for char in enc if char != \\' \\'] for enc in encodings]\\n        sinhala_lengths = [tok[1] for tok in results]\\n        return [l / len(enc) for enc, l in zip(encodings, sinhala_lengths)]', 'file_name': 'preprocessing.py', 'type': 'function', 'uses': 'process_text_with_token_counts, remove_non_printable'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ft(INDEX_NAME).search(\n",
    "    vector_search_query_with_range,\n",
    "    {\n",
    "      'query_vector': np.array(encoded_queries[0], dtype=np.float32).tobytes(),\n",
    "      'range': 1.0\n",
    "    }\n",
    ").docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
